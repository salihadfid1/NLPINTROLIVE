{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 22 23:11:39 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "## solution to exercise at end of chapter 3\n",
    "#first question\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 19 10:14:08 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import WordCloud\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    #punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"] \n",
    "    punctuation='!?,.:;\"\\')(_-'\n",
    "    text_without_punctuations = []\n",
    "    for entity in tokens:\n",
    "        newstring = \"\"\n",
    "        for char in entity:\n",
    "            if(char not in punctuation):\n",
    "                  newstring+= char\n",
    "        text_without_punctuations.append(newstring)\n",
    "    return text_without_punctuations\n",
    "  \n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    \n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def preprocess(pstr1):\n",
    "    \n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    " \n",
    "    \n",
    "raw_data = pd.read_csv(\"C:/IR Course/NLP_Intro/SMSSpamCollection.csv\",  encoding='iso-8859-1') \n",
    "\n",
    "\n",
    "def populatedictcorpus(data):\n",
    "    pdict1 = {} \n",
    "    textspam = \"\"\n",
    "    textham = \"\"\n",
    "    list_WtV_spam=[]\n",
    "    list_WtV_ham=[]\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "            if row['Email']=='spam':\n",
    "                                      s= preprocess(row['Description'])\n",
    "                                      textspam = textspam + \" \".join(s) \n",
    "                                      textspam = row['Description'] + \" \" +  textspam\n",
    "                                      list_WtV_spam.append(row['Description'].split(\" \")) \n",
    " \n",
    "            else:   \n",
    "                                      s= preprocess(row['Description'])\n",
    "                                      textham = textham + \" \".join(s) \n",
    "                                      textham = row['Description'] + \" \" +  textham\n",
    "                                      list_WtV_ham.append(row['Description'].split(\" \")) \n",
    "                                      \n",
    "                                      \n",
    "   \n",
    "    pdict1.update({'spam': textspam})\n",
    "    pdict1.update({'ham': textham})\n",
    "   \n",
    "    alldata = [pdict1,list_WtV_spam, list_WtV_ham ]\n",
    "   \n",
    "    return alldata\n",
    "\n",
    "\n",
    "def freqalltokens(palltext):\n",
    "        \n",
    "            dictcounts = {}\n",
    "            palltext = palltext.split(\" \")\n",
    "            for token in palltext:\n",
    "                if token in dictcounts:\n",
    "                    dictcounts[token] = dictcounts[token] + 1\n",
    "                else:\n",
    "                     dictcounts[token] =  1\n",
    "            sorted_val = sorted(dictcounts.items(), key=operator.itemgetter(1), reverse=True)         \n",
    "            return sorted_val      \n",
    "        \n",
    "    \n",
    "    \n",
    "count_spamham = []\n",
    "sum_tokens=0\n",
    "alltext = \"\"\n",
    "complete_list = populatedictcorpus(raw_data)\n",
    "dict1 = complete_list[0]\n",
    "for key in dict1:\n",
    "    count_spamham.append([key, len(dict1[key])])\n",
    "    sum_tokens=len(dict1[key].split(\" \")) + sum_tokens\n",
    "    alltext = alltext + dict1[key]\n",
    "\n",
    "a=freqalltokens(alltext)\n",
    "      \n",
    "print (a)\n",
    "\n",
    "#birthday = 15\n",
    "#normlised for birthday\n",
    "\n",
    "normalfreq = 15/sum_tokens *1000\n",
    "print (normalfreq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
